{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Códigos do Slide PLN 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    data = ['Este eh um cachorro','Este eh um gato','Eu amo meu gato','Este eh meu nome']    \n",
    "    dat=[]\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "        print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "    listOfBigrams = []\n",
    "    bigramCounts = {}\n",
    "    unigramCounts = {}\n",
    "    for i in range(len(data)-1):\n",
    "        if i < len(data) - 1 and data[i+1].islower():\n",
    "            listOfBigrams.append((data[i], data[i + 1]))\n",
    "            if (data[i], data[i+1]) in bigramCounts:\n",
    "                bigramCounts[(data[i], data[i + 1])] += 1\n",
    "            else:\n",
    "                bigramCounts[(data[i], data[i + 1])] = 1\n",
    "        if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "        else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "    return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readData()\n",
    "listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "print(\"\\n Todos os bigramas possiveis sao: \")\n",
    "print(listOfBigrams)\n",
    "\n",
    "print(\"\\n Bigramas e suas frequencias: \")\n",
    "print(bigramCounts)\n",
    "\n",
    "print(\"\\n Unigramas e suas frequencias: \")\n",
    "print(unigramCounts)\n",
    "\n",
    "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "print(\"\\n Bigramas e suas probabilidades: \")\n",
    "print(bigramProb)\n",
    "inputList=\"Este eh meu gato\"\n",
    "splt=inputList.split()\n",
    "outputProb1 = 1\n",
    "bilist=[]\n",
    "bigrm=[]\n",
    "\n",
    "for i in range(len(splt) - 1):\n",
    "    if i < len(splt) - 1:\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "    \n",
    "print(\"\\n Os bigramas na sentenca de entrada sao: \")\n",
    "print(bilist)\n",
    "for i in range(len(bilist)):\n",
    "    if bilist[i] in bigramProb:\n",
    "        outputProb1 *= bigramProb[bilist[i]]\n",
    "    else:\n",
    "        outputProb1 *= 0\n",
    "print('\\n' + 'Probabilidade da sentenca '+inputList+ '= ' + str(outputProb1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo Prático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from numpy.random import choice \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram():\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "        self.context = defaultdict(Counter)\n",
    "        self.start_count = 0\n",
    "        self.token_count = 0\n",
    "        self.vocab_count = 0\n",
    "    \n",
    "    def convert_sentence(self, sentence):\n",
    "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "    \n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
    "                self.unigram_counts[word] += 1\n",
    "            self.start_count += 1\n",
    "            \n",
    "        # collect bigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "            for bigram in bigram_list:\n",
    "                self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "                self.context[bigram[1]][bigram[0]] += 1\n",
    "        self.token_count = sum(self.unigram_counts.values())\n",
    "        self.vocab_count = len(self.unigram_counts.keys())\n",
    "        \n",
    "    def generate_sentence(self):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        while current_word != \"</s>\":\n",
    "            prev_word = current_word\n",
    "            prev_word_counts = self.bigram_counts[prev_word]\n",
    "            # obtain bigram probability distribution given the previous word\n",
    "            bigram_probs = []\n",
    "            total_counts = float(sum(prev_word_counts.values()))\n",
    "            for word in prev_word_counts:\n",
    "                bigram_probs.append(prev_word_counts[word] / total_counts)\n",
    "            # sample the next word\n",
    "            current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
    "            sentence.append(current_word)\n",
    "            \n",
    "        sentence = \" \".join(sentence[1:-1])\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Bigram()\n",
    "bigram.get_counts(brown.sents())\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(bigram.generate_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test():\n",
    "    sents = list(brown.sents())\n",
    "    shuffle(sents)\n",
    "    cutoff = int(0.8*len(sents))\n",
    "    training_set = sents[:cutoff]\n",
    "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "    return training_set, test_set\n",
    "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in tqdm(sentences):\n",
    "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
    "    return math.exp(-total_log_prob / test_token_count)\n",
    "training_set, test_set = split_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "# baixar o modelo: POS_tagger_brill.pkl\n",
    "# https://github.com/inoueMashuu/POS-tagger-portuguese-nltk/tree/master/trained_POS_taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapta aqui pra sua pasta depois de baixar o repositório da célula anterior\n",
    "folder = 'C:/Users/dealbuqc/Desktop/UFPB/Classes/SBC/'\n",
    "teste_tagger = joblib.load(folder+'POS_tagger_brill.pkl.pkl')\n",
    "phrase = 'O rato roeu a roupa do rei de Roma'\n",
    "teste_tagger.tag(word_tokenize(phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0] [st] = {\"prob\": start_p[st] * emit_p[st] [obs[0]], \"prev\": None}\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1] [states[0]] [\"prob\"] * trans_p[states[0]] [st] * emit_p[st] [obs[t]]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1] [prev_st] [\"prob\"] * trans_p[prev_st] [st] * emit_p[st] [obs[t]]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "            max_prob = max_tr_prob\n",
    "            V[t] [st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1] [previous] [\"prev\"])\n",
    "        previous = V[t + 1] [previous] [\"prev\"]\n",
    "    print (\"A sequencia de estados foi: \" + \" \".join(opt) + \" tendo probabilidade de %s\" % max_prob + \" (maior probabilidade)\")\n",
    "\n",
    "def dptable(V):\n",
    "    # Print a table of steps from dictionary\n",
    "    yield \" \" * 5 + \"     \".join((\"%3d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%lf\" % v[state] [\"prob\"]) for v in V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the states (POS tags) and observation symbols (words)\n",
    "states = ['NOUN', 'VERB', 'ADJ']\n",
    "observations = ['dog', 'runs', 'fast']\n",
    "# Define the transition probabilities\n",
    "transition_probabilities = {\n",
    "    'NOUN': {'NOUN': 0.1, 'VERB': 0.7, 'ADJ': 0.2},\n",
    "    'VERB': {'NOUN': 0.3, 'VERB': 0.4, 'ADJ': 0.3},\n",
    "    'ADJ': {'NOUN': 0.5, 'VERB': 0.1, 'ADJ': 0.4}\n",
    " }\n",
    "# Define the emission probabilities\n",
    "emission_probabilities = {\n",
    "    'NOUN': {'dog': 0.6, 'runs': 0.1, 'fast': 0.3},\n",
    "    'VERB': {'dog': 0.1, 'runs': 0.7, 'fast': 0.2},\n",
    "    'ADJ': {'dog': 0.3, 'runs': 0.2, 'fast': 0.5}\n",
    " }\n",
    "# Define the initial probabilities\n",
    "start_probabilities = {'NOUN': 0.4, 'VERB': 0.3, 'ADJ': 0.3}\n",
    "viterbi(observations,states, start_probabilities, transition_probabilities, emission_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engenharia de Características "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_capital_letters(text):\n",
    "    return sum(1 for char in text if char.isupper())\n",
    "\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "\n",
    "def count_punctuations(text):\n",
    "# retorna um dicinario com 32 pontuacoes e contadores associados\n",
    "    punctuations=\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d\n",
    "\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall('\"([^\"]*)\"', text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x) \n",
    "\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('portuguese'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ó mar salgado, quanto do teu sal, são lágrimas de Portugal! Por te cruzarmos, quantas mães choraram, quantos filhos em vão rezaram! Quantas noivas ficaram por casar, para que fosses nosso, ó mar! Valeu a pena? Tudo vale a pena, se a alma não é pequena. Quem quer passar além do Bojador, tem que passar além da dor. Deus ao mar o perigo e o abismo deu. Mas nele é que espelhou o céu.\"\n",
    "\n",
    "dict_features = {}\n",
    "\n",
    "dict_features['char_count'] = count_chars(text)\n",
    "dict_features['word_count'] = count_words(text)\n",
    "dict_features['stopword_count'] = count_stopwords(text)\n",
    "dict_features['unique_word_count'] = count_unique_words(text)\n",
    "dict_features['sent_count'] = count_sent(text)\n",
    "\n",
    "dict_features['avg_wordlength'] = int(dict_features['char_count'])/int(dict_features['word_count'])\n",
    "dict_features['avg_sentlength'] = dict_features['word_count']/dict_features['sent_count']\n",
    "dict_features['unique_vs_words'] = dict_features['unique_word_count']/dict_features['word_count']\n",
    "dict_features['stopwords_vs_words'] = dict_features['stopword_count']/dict_features['word_count'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
